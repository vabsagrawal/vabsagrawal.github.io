{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Java oops Concepts Encapsulation Encapsulation is one of the four fundamental OOP concepts. The other three are inheritance, polymorphism, and abstraction. Encapsulation in Java is a mechanism of wrapping the data (variables) and code acting on the data (methods) together as as single unit. In encapsulation the variables of a class will be hidden from other classes, and can be accessed only through the methods of their current class, therefore it is also known as data hiding. To achieve encapsulation in Java : Declare the variables of a class as private. Provide public setter and getter methods to modify and view the variables values. Inheritance Inheritance can be defined as the process where one class acquires the properties (methods and fields) of another. With the use of inheritance the information is made manageable in a hierarchical order. The class which inherits the properties of other is known as subclass (derived class, child class) and the class whose properties are inherited is known as superclass (base class, parent class). extends is the keyword used to inherit the properties of a class. Below given is the syntax of extends keyword. Polymorphism Polymorphism is the concept where an object behaves differently in different situations. There are two types of polymorphism \u2013 compile time polymorphism and runtime polymorphism. Compile time polymorphism is achieved by method overloading. For example, we can have a class as below. public class Circle { public void draw(){ System.out.println(\"Drwaing circle with default color Black and diameter 1 cm.\"); } public void draw(int diameter){ System.out.println(\"Drwaing circle with default color Black and diameter\"+diameter+\" cm.\"); } public void draw(int diameter, String color){ System.out.println(\"Drwaing circle with color\"+color+\" and diameter\"+diameter+\" cm.\"); } } Here we have multiple draw methods but they have different behavior. This is a case of method overloading because all the methods name is same and arguments are different. Here compiler will be able to identify the method to invoke at compile time, hence it\u2019s called compile time polymorphism. Runtime polymorphism is implemented when we have \u201cIS-A\u201d relationship between objects. This is also called as method overriding because subclass has to override the superclass method for runtime polymorphism. If we are working in terms of superclass, the actual implementation class is decided at runtime. Compiler is not able to decide which class method will be invoked. This decision is done at runtime, hence the name as runtime polymorphism or dynamic method dispatch. package com.test; public interface Shape { public void draw(); } package com.test; public class Circle implements Shape{ @Override public void draw(){ System.out.println(\"Drwaing circle\"); } } package com.test; public class Square implements Shape { @Override public void draw() { System.out.println(\"Drawing Square\"); } } Shape is the superclass and there are two subclasses Circle and Square. Below is an example of runtime polymorphism. Shape sh = new Circle(); sh.draw(); Shape sh1 = getShape(); //some third party logic to determine shape sh1.draw(); In above examples, java compiler don\u2019t know the actual implementation class of Shape that will be used at runtime, hence runtime polymorphism. Abstraction In Object oriented programming Abstraction is a process process of hiding the implementation details from the user, only the functionality will be provided to the user. In other words user will have the information on what the object does instead of how it does it. In Java Abstraction is achieved using Abstract classes and Interfaces. A class which contains the abstract keyword in its declaration is known as abstract class. An interface is a reference type in Java, it is similar to class, it is a collection of abstract methods. A class implements an interface, thereby inheriting the abstract methods of the interface. Along with abstract methods an interface may also contain constants, default methods, static methods, and nested types. Method bodies exist only for default methods and static methods. Writing an interface is similar to writing a class. But a class describes the attributes and behaviours of an object. And an interface contains behaviours that a class implements. Unless the class that implements the interface is abstract, all the methods of the interface need to be defined in the class. An interface is similar to a class in the following ways: An interface can contain any number of methods. An interface is written in a file with a .java extension, with the name of the interface matching the name of the file. The byte code of an interface appears in a .class file. Interfaces appear in packages, and their corresponding bytecode file must be in a directory structure that matches the package name. However, an interface is different from a class in several ways, including: You cannot instantiate an interface. An interface does not contain any constructors. All of the methods in an interface are abstract. An interface cannot contain instance fields. The only fields that can appear in an interface must be declared both static and final. An interface is not extended by a class; it is implemented by a class. An interface can extend multiple interfaces. Refrences https://www.google.com https://www.tutorialspoint.com","title":"Java oops Concepts"},{"location":"#java-oops-concepts","text":"","title":"Java oops Concepts"},{"location":"#encapsulation","text":"Encapsulation is one of the four fundamental OOP concepts. The other three are inheritance, polymorphism, and abstraction. Encapsulation in Java is a mechanism of wrapping the data (variables) and code acting on the data (methods) together as as single unit. In encapsulation the variables of a class will be hidden from other classes, and can be accessed only through the methods of their current class, therefore it is also known as data hiding. To achieve encapsulation in Java : Declare the variables of a class as private. Provide public setter and getter methods to modify and view the variables values.","title":"Encapsulation"},{"location":"#inheritance","text":"Inheritance can be defined as the process where one class acquires the properties (methods and fields) of another. With the use of inheritance the information is made manageable in a hierarchical order. The class which inherits the properties of other is known as subclass (derived class, child class) and the class whose properties are inherited is known as superclass (base class, parent class). extends is the keyword used to inherit the properties of a class. Below given is the syntax of extends keyword.","title":"Inheritance"},{"location":"#polymorphism","text":"Polymorphism is the concept where an object behaves differently in different situations. There are two types of polymorphism \u2013 compile time polymorphism and runtime polymorphism. Compile time polymorphism is achieved by method overloading. For example, we can have a class as below. public class Circle { public void draw(){ System.out.println(\"Drwaing circle with default color Black and diameter 1 cm.\"); } public void draw(int diameter){ System.out.println(\"Drwaing circle with default color Black and diameter\"+diameter+\" cm.\"); } public void draw(int diameter, String color){ System.out.println(\"Drwaing circle with color\"+color+\" and diameter\"+diameter+\" cm.\"); } } Here we have multiple draw methods but they have different behavior. This is a case of method overloading because all the methods name is same and arguments are different. Here compiler will be able to identify the method to invoke at compile time, hence it\u2019s called compile time polymorphism. Runtime polymorphism is implemented when we have \u201cIS-A\u201d relationship between objects. This is also called as method overriding because subclass has to override the superclass method for runtime polymorphism. If we are working in terms of superclass, the actual implementation class is decided at runtime. Compiler is not able to decide which class method will be invoked. This decision is done at runtime, hence the name as runtime polymorphism or dynamic method dispatch. package com.test; public interface Shape { public void draw(); } package com.test; public class Circle implements Shape{ @Override public void draw(){ System.out.println(\"Drwaing circle\"); } } package com.test; public class Square implements Shape { @Override public void draw() { System.out.println(\"Drawing Square\"); } } Shape is the superclass and there are two subclasses Circle and Square. Below is an example of runtime polymorphism. Shape sh = new Circle(); sh.draw(); Shape sh1 = getShape(); //some third party logic to determine shape sh1.draw(); In above examples, java compiler don\u2019t know the actual implementation class of Shape that will be used at runtime, hence runtime polymorphism.","title":"Polymorphism"},{"location":"#abstraction","text":"In Object oriented programming Abstraction is a process process of hiding the implementation details from the user, only the functionality will be provided to the user. In other words user will have the information on what the object does instead of how it does it. In Java Abstraction is achieved using Abstract classes and Interfaces. A class which contains the abstract keyword in its declaration is known as abstract class. An interface is a reference type in Java, it is similar to class, it is a collection of abstract methods. A class implements an interface, thereby inheriting the abstract methods of the interface. Along with abstract methods an interface may also contain constants, default methods, static methods, and nested types. Method bodies exist only for default methods and static methods. Writing an interface is similar to writing a class. But a class describes the attributes and behaviours of an object. And an interface contains behaviours that a class implements. Unless the class that implements the interface is abstract, all the methods of the interface need to be defined in the class. An interface is similar to a class in the following ways: An interface can contain any number of methods. An interface is written in a file with a .java extension, with the name of the interface matching the name of the file. The byte code of an interface appears in a .class file. Interfaces appear in packages, and their corresponding bytecode file must be in a directory structure that matches the package name. However, an interface is different from a class in several ways, including: You cannot instantiate an interface. An interface does not contain any constructors. All of the methods in an interface are abstract. An interface cannot contain instance fields. The only fields that can appear in an interface must be declared both static and final. An interface is not extended by a class; it is implemented by a class. An interface can extend multiple interfaces.","title":"Abstraction"},{"location":"#refrences","text":"https://www.google.com https://www.tutorialspoint.com","title":"Refrences"},{"location":"jvmarch/","text":"JVM Architecture Every Java developer knows that bytecode will be executed by JRE (Java Runtime Environment). But many doesn't know the fact that JRE is the implementation of Java Virtual Machine (JVM), which analyzes the bytecode, interprets the code, and executes it. It is very important as a developer that we should know the Architecture of the JVM, as it enables us to write code more efficiently. In this article, we will learn more dJVM Architectureeeply about the JVM architecture in Java and the different components of the JVM. What is the JVM? A Virtual Machine is a software implementation of a physical machine. Java was developed with the concept of WORA (Write Once Run Anywhere), which runs on a VM. The compiler compiles the Java file into a Java .class file, then that .class file is input into the JVM, which Loads and executes the class file. Below is a diagram of the Architecture of the JVM. JVM Architecture Diagram How Does the JVM Work? As shown in the above architecture diagram, the JVM is divided into three main subsystems: Class Loader Subsystem Runtime Data Area Execution Engine 1. Class Loader Subsystem Java's dynamic class loading functionality is handled by the class loader subsystem. It loads, links. and initializes the class file when it refers to a class for the first time at runtime, not compile time. 1.1 Loading Classes will be loaded by this component. Boot Strap class Loader, Extension class Loader, and Application class Loader are the three class loader which will help in achieving it. Boot Strap ClassLoader \u2013 Responsible for loading classes from the bootstrap classpath, nothing but rt.jar. Highest priority will be given to this loader. Extension ClassLoader \u2013 Responsible for loading classes which are inside ext folder (jre\\lib). Application ClassLoader \u2013Responsible for loading Application Level Classpath, path mentioned Environment Variable etc. The above Class Loaders will follow Delegation Hierarchy Algorithm while loading the class files. 1.2 Linking Verify \u2013 Bytecode verifier will verify whether the generated bytecode is proper or not if verification fails we will get the verification error. Prepare \u2013 For all static variables memory will be allocated and assigned with default values. Resolve \u2013 All symbolic memory references are replaced with the original references from Method Area. 1.3 Initialization This is the final phase of Class Loading, here all static variables will be assigned with the original values, and the static block will be executed. 2. Runtime Data Area The Runtime Data Area is divided into 5 major components: Method Area \u2013 All the class level data will be stored here, including static variables. There is only one method area per JVM, and it is a shared resource. Heap Area \u2013 All the Objects and their corresponding instance variables and arrays will be stored here. There is also one Heap Area per JVM. Since the Method and Heap areas share memory for multiple threads, the data stored is not thread safe. Stack Area \u2013 For every thread, a separate runtime stack will be created. For every method call, one entry will be made in the stack memory which is called as Stack Frame. All local variables will be created in the stack memory. The stack area is thread safe since it is not a shared resource. The Stack Frame is divided into three subentities: Local Variable Array \u2013 Related to the method how many local variables are involved and the corresponding values will be stored here. Operand stack \u2013 If any intermediate operation is required to perform, operand stack acts as runtime workspace to perform the operation. Frame data \u2013 All symbols corresponding to the method is stored here. In the case of any exception, the catch block information will be maintained in the frame data. PC Registers \u2013 Each thread will have separate PC Registers, to hold the address of current executing instruction once the instruction is executed the PC register will be updated with the next instruction. Native Method stacks \u2013 Native Method Stack holds native method information. For every thread, a separate native method stack will be created. 3. Execution Engine The bytecode which is assigned to the Runtime Data Area will be executed by the Execution Engine. The Execution Engine reads the bytecode and executes it piece by piece. Interpreter \u2013 The interpreter interprets the bytecode faster, but executes slowly. The disadvantage of the interpreter is that when one method is called multiple times, every time a new interpretation is required. JIT Compiler \u2013 The JIT Compiler neutralizes the disadvantage of the interpreter. The Execution Engine will be using the help of the interpreter in converting byte code, but when it finds repeated code it uses the JIT compiler, which compiles the entire bytecode and changes it to native code. This native code will be used directly for repeated method calls, which improve the performance of the system. Intermediate Code generator \u2013 Produces intermediate code Code Optimizer \u2013 Responsible for optimizing the intermediate code generated above Target Code Generator \u2013 Responsible for Generating Machine Code or Native Code Profiler \u2013 A special component, responsible for finding hotspots, i.e. whether the method is called multiple times or not. Garbage Collector: Collects and removes unreferenced objects. Garbage Collection can be triggered by calling \"System.gc()\", but the execution is not guaranteed. Garbage collection of the JVM collects the objects that are created. Java Native Interface (JNI): JNI will be interacting with the Native Method Libraries and provides the Native Libraries required for the Execution Engine. Native Method Libraries:It is a collection of the Native Libraries which is required for the Execution Engine. Refrences https://dzone.com/articles/jvm-architecture-explained","title":"JVM Architecture"},{"location":"jvmarch/#jvm-architecture","text":"Every Java developer knows that bytecode will be executed by JRE (Java Runtime Environment). But many doesn't know the fact that JRE is the implementation of Java Virtual Machine (JVM), which analyzes the bytecode, interprets the code, and executes it. It is very important as a developer that we should know the Architecture of the JVM, as it enables us to write code more efficiently. In this article, we will learn more dJVM Architectureeeply about the JVM architecture in Java and the different components of the JVM.","title":"JVM Architecture"},{"location":"jvmarch/#what-is-the-jvm","text":"A Virtual Machine is a software implementation of a physical machine. Java was developed with the concept of WORA (Write Once Run Anywhere), which runs on a VM. The compiler compiles the Java file into a Java .class file, then that .class file is input into the JVM, which Loads and executes the class file. Below is a diagram of the Architecture of the JVM. JVM Architecture Diagram","title":"What is the JVM?"},{"location":"jvmarch/#how-does-the-jvm-work","text":"As shown in the above architecture diagram, the JVM is divided into three main subsystems: Class Loader Subsystem Runtime Data Area Execution Engine 1. Class Loader Subsystem Java's dynamic class loading functionality is handled by the class loader subsystem. It loads, links. and initializes the class file when it refers to a class for the first time at runtime, not compile time. 1.1 Loading Classes will be loaded by this component. Boot Strap class Loader, Extension class Loader, and Application class Loader are the three class loader which will help in achieving it. Boot Strap ClassLoader \u2013 Responsible for loading classes from the bootstrap classpath, nothing but rt.jar. Highest priority will be given to this loader. Extension ClassLoader \u2013 Responsible for loading classes which are inside ext folder (jre\\lib). Application ClassLoader \u2013Responsible for loading Application Level Classpath, path mentioned Environment Variable etc. The above Class Loaders will follow Delegation Hierarchy Algorithm while loading the class files. 1.2 Linking Verify \u2013 Bytecode verifier will verify whether the generated bytecode is proper or not if verification fails we will get the verification error. Prepare \u2013 For all static variables memory will be allocated and assigned with default values. Resolve \u2013 All symbolic memory references are replaced with the original references from Method Area. 1.3 Initialization This is the final phase of Class Loading, here all static variables will be assigned with the original values, and the static block will be executed. 2. Runtime Data Area The Runtime Data Area is divided into 5 major components: Method Area \u2013 All the class level data will be stored here, including static variables. There is only one method area per JVM, and it is a shared resource. Heap Area \u2013 All the Objects and their corresponding instance variables and arrays will be stored here. There is also one Heap Area per JVM. Since the Method and Heap areas share memory for multiple threads, the data stored is not thread safe. Stack Area \u2013 For every thread, a separate runtime stack will be created. For every method call, one entry will be made in the stack memory which is called as Stack Frame. All local variables will be created in the stack memory. The stack area is thread safe since it is not a shared resource. The Stack Frame is divided into three subentities: Local Variable Array \u2013 Related to the method how many local variables are involved and the corresponding values will be stored here. Operand stack \u2013 If any intermediate operation is required to perform, operand stack acts as runtime workspace to perform the operation. Frame data \u2013 All symbols corresponding to the method is stored here. In the case of any exception, the catch block information will be maintained in the frame data. PC Registers \u2013 Each thread will have separate PC Registers, to hold the address of current executing instruction once the instruction is executed the PC register will be updated with the next instruction. Native Method stacks \u2013 Native Method Stack holds native method information. For every thread, a separate native method stack will be created. 3. Execution Engine The bytecode which is assigned to the Runtime Data Area will be executed by the Execution Engine. The Execution Engine reads the bytecode and executes it piece by piece. Interpreter \u2013 The interpreter interprets the bytecode faster, but executes slowly. The disadvantage of the interpreter is that when one method is called multiple times, every time a new interpretation is required. JIT Compiler \u2013 The JIT Compiler neutralizes the disadvantage of the interpreter. The Execution Engine will be using the help of the interpreter in converting byte code, but when it finds repeated code it uses the JIT compiler, which compiles the entire bytecode and changes it to native code. This native code will be used directly for repeated method calls, which improve the performance of the system. Intermediate Code generator \u2013 Produces intermediate code Code Optimizer \u2013 Responsible for optimizing the intermediate code generated above Target Code Generator \u2013 Responsible for Generating Machine Code or Native Code Profiler \u2013 A special component, responsible for finding hotspots, i.e. whether the method is called multiple times or not. Garbage Collector: Collects and removes unreferenced objects. Garbage Collection can be triggered by calling \"System.gc()\", but the execution is not guaranteed. Garbage collection of the JVM collects the objects that are created. Java Native Interface (JNI): JNI will be interacting with the Native Method Libraries and provides the Native Libraries required for the Execution Engine. Native Method Libraries:It is a collection of the Native Libraries which is required for the Execution Engine.","title":"How Does the JVM Work?"},{"location":"jvmarch/#refrences","text":"https://dzone.com/articles/jvm-architecture-explained","title":"Refrences"},{"location":"page1/","text":"Page1","title":"Page1"},{"location":"page1/#page1","text":"","title":"Page1"},{"location":"rssvsz/","text":"Analogy-1 RSS is the Resident Set Size and is used to show how much memory is allocated to that process and is in RAM. It does not include memory that is swapped out. It does include memory from shared libraries as long as the pages from those libraries are actually in memory. It does include all stack and heap memory. VSZ is the Virtual Memory Size. It includes all memory that the process can access, including memory that is swapped out, memory that is allocated, but not used, and memory that is from shared libraries. So if process A has a 500K binary and is linked to 2500K of shared libraries, has 200K of stack/heap allocations of which 100K is actually in memory (rest is swapped or unused), and it has only actually loaded 1000K of the shared libraries and 400K of its own binary then: RSS: 400K + 1000K + 100K = 1500K VSZ: 500K + 2500K + 200K = 3200K Since part of the memory is shared, many processes may use it, so if you add up all of the RSS values you can easily end up with more space than your system has. The memory that is allocated also may not be in RSS until it is actually used by the program. So if your program allocated a bunch of memory up front, then uses it over time, you could see RSS going up and VSZ staying the same. There is also PSS (proportional set size). This is a newer measure which tracks the shared memory as a proportion used by the current process. So if there were two processes using the same shared library from before: PSS: 400K + (1000K/2) + 100K = 400K + 500K + 100K = 1000K Threads all share the same address space, so the RSS, VSZ and PSS for each thread is identical to all of the other threads in the process. Use ps or top to view this information in linux/unix. Analogy-2 This article explains three indicators that can possibly be used to measure the memory consumption of a single process on Linux. VSZ (Virtual Memory Size), RSS (Resident Set Size), and PSS (Proportional Set Size). Although this will lack accuracy, let us consider an allegory to get the idea. There are three people sharing a room. We will consider each person to represent a process, and living expenses to represent memory consumption. Measuring the memory consumption of a single process will be represented as calculating the total living expense for one person in this allegory. Each person has their own cell phone line that is not being shared. All three indicators, VSZ, RSS, and PSS, will count the cell phone bills as each persons living expense individually, and there is no problem with this. The shared room comes with a garage space that can be used if they pay for it, but they all don't drive and they are not using it. However, VSZ will count the entire garage space cost as each persons living expense even though they are not using it. VSZ, therefore, represents the total living expense when they spend on every possible thing regardless of the actual usage. RSS and PSS only count expenses that are actually being used, and therefore, they will not count the garage space cost because it is not being used. Since they are sharing the internet connection and the cable TV, they split up those bills. However, RSS will count the entire amount of the internet connection and cable TV as each persons living expense, even though they are sharing it and splitting up the bill. The idea of RSS is to calculate living expenses as it was not shared with anybody else. PSS will only count one third of the internet connection and cable TV bill as each persons living expense, because they are sharing it. This is more reasonable than RSS since it is considers the fact that they are sharing it. Let's make the allegory a little bit more complicated in order to understand the limitations of PSS. One person is always on the internet and doesn't watch TV that much. Hence, that person pays 50% of the internet connection bill and 20% of the cable TV bill upon agreement. PSS, however, cannot handle situations like this. It will simply calculate one third of the internet connection bill and cable TV bill as each persons living expense. Using PSS is what I consider most reasonable. However, there are limitations and there are also situations where RSS may work better. RSS is reasonable when you want to know the total living expense when you move out and live on your own. Refrences https://www.google.com https://web.archive.org/web/20120520221529/http://emilics.com/blog/article/mconsumption.html http://manpages.ubuntu.com/manpages/en/man1/ps.1.html https://web.archive.org/web/20120520221529/http://emilics.com/blog/article/mconsumption.html","title":"Memory RSS vs VSZ"},{"location":"rssvsz/#analogy-1","text":"RSS is the Resident Set Size and is used to show how much memory is allocated to that process and is in RAM. It does not include memory that is swapped out. It does include memory from shared libraries as long as the pages from those libraries are actually in memory. It does include all stack and heap memory. VSZ is the Virtual Memory Size. It includes all memory that the process can access, including memory that is swapped out, memory that is allocated, but not used, and memory that is from shared libraries. So if process A has a 500K binary and is linked to 2500K of shared libraries, has 200K of stack/heap allocations of which 100K is actually in memory (rest is swapped or unused), and it has only actually loaded 1000K of the shared libraries and 400K of its own binary then: RSS: 400K + 1000K + 100K = 1500K VSZ: 500K + 2500K + 200K = 3200K Since part of the memory is shared, many processes may use it, so if you add up all of the RSS values you can easily end up with more space than your system has. The memory that is allocated also may not be in RSS until it is actually used by the program. So if your program allocated a bunch of memory up front, then uses it over time, you could see RSS going up and VSZ staying the same. There is also PSS (proportional set size). This is a newer measure which tracks the shared memory as a proportion used by the current process. So if there were two processes using the same shared library from before: PSS: 400K + (1000K/2) + 100K = 400K + 500K + 100K = 1000K Threads all share the same address space, so the RSS, VSZ and PSS for each thread is identical to all of the other threads in the process. Use ps or top to view this information in linux/unix.","title":"Analogy-1"},{"location":"rssvsz/#analogy-2","text":"This article explains three indicators that can possibly be used to measure the memory consumption of a single process on Linux. VSZ (Virtual Memory Size), RSS (Resident Set Size), and PSS (Proportional Set Size). Although this will lack accuracy, let us consider an allegory to get the idea. There are three people sharing a room. We will consider each person to represent a process, and living expenses to represent memory consumption. Measuring the memory consumption of a single process will be represented as calculating the total living expense for one person in this allegory. Each person has their own cell phone line that is not being shared. All three indicators, VSZ, RSS, and PSS, will count the cell phone bills as each persons living expense individually, and there is no problem with this. The shared room comes with a garage space that can be used if they pay for it, but they all don't drive and they are not using it. However, VSZ will count the entire garage space cost as each persons living expense even though they are not using it. VSZ, therefore, represents the total living expense when they spend on every possible thing regardless of the actual usage. RSS and PSS only count expenses that are actually being used, and therefore, they will not count the garage space cost because it is not being used. Since they are sharing the internet connection and the cable TV, they split up those bills. However, RSS will count the entire amount of the internet connection and cable TV as each persons living expense, even though they are sharing it and splitting up the bill. The idea of RSS is to calculate living expenses as it was not shared with anybody else. PSS will only count one third of the internet connection and cable TV bill as each persons living expense, because they are sharing it. This is more reasonable than RSS since it is considers the fact that they are sharing it. Let's make the allegory a little bit more complicated in order to understand the limitations of PSS. One person is always on the internet and doesn't watch TV that much. Hence, that person pays 50% of the internet connection bill and 20% of the cable TV bill upon agreement. PSS, however, cannot handle situations like this. It will simply calculate one third of the internet connection bill and cable TV bill as each persons living expense. Using PSS is what I consider most reasonable. However, there are limitations and there are also situations where RSS may work better. RSS is reasonable when you want to know the total living expense when you move out and live on your own.","title":"Analogy-2"},{"location":"rssvsz/#refrences","text":"https://www.google.com https://web.archive.org/web/20120520221529/http://emilics.com/blog/article/mconsumption.html http://manpages.ubuntu.com/manpages/en/man1/ps.1.html https://web.archive.org/web/20120520221529/http://emilics.com/blog/article/mconsumption.html","title":"Refrences"},{"location":"sub-page1/","text":"sub-page1","title":"sub-page1"},{"location":"sub-page1/#sub-page1","text":"","title":"sub-page1"}]}